{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short 2\n",
    "\n",
    "### Ensamble model of AutoGluon and Catboost\n",
    "\n",
    "Name: Erlend Lokna, Student ID: 528564\n",
    "\n",
    "Name: Johan Vik Mathisen, Student ID: 508258\n",
    "\n",
    "\n",
    "Team name: Shaky Warriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import catboost as cb\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        kind: observerd, estimated, train\n",
    "        \"\"\"\n",
    "\n",
    "        train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "        train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "        train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "        # Estimated training data for each location\n",
    "        X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet')\n",
    "        X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet')\n",
    "        X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet')\n",
    "\n",
    "        # Observed training data for each location\n",
    "        X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet')\n",
    "        X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet')\n",
    "        X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet')\n",
    "\n",
    "        # Estimated test data for each location\n",
    "        X_test_estimated_b = pd.read_parquet('data/B/X_test_estimated.parquet')\n",
    "        X_test_estimated_a = pd.read_parquet('data/A/X_test_estimated.parquet')\n",
    "        X_test_estimated_c = pd.read_parquet('data/C/X_test_estimated.parquet')\n",
    "\n",
    "        Y_train = {\n",
    "            'a': train_a, \n",
    "            'b':train_b, \n",
    "            'c':train_c\n",
    "        }\n",
    "        X_train_estimated = {\n",
    "            'a':X_train_estimated_a,\n",
    "            'b':X_train_estimated_b,\n",
    "            'c':X_train_estimated_c\n",
    "        }\n",
    "        X_train_observed = {\n",
    "            'a':X_train_observed_a,\n",
    "            'b':X_train_observed_b,\n",
    "            'c':X_train_observed_c\n",
    "        }\n",
    "        X_test_estimated = {\n",
    "            'a':X_test_estimated_a,\n",
    "            'b':X_test_estimated_b,\n",
    "            'c':X_test_estimated_c\n",
    "        }\n",
    "        self.X_train_observed =  X_train_observed\n",
    "        self.X_train_estimated = X_train_estimated\n",
    "        self.X_test_estimated = X_test_estimated\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def resample_to_hourly(self):\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = to_hourly(self.X_train_observed[loc])\n",
    "            self.X_train_estimated[loc] = to_hourly(self.X_train_estimated[loc])\n",
    "            self.X_test_estimated[loc] = to_hourly(self.X_test_estimated[loc])\n",
    "\n",
    "\n",
    "    def select_features(self, features):\n",
    "        \"\"\" \n",
    "        Reduces dim by selecting only features from \"features\"\n",
    "        This will remove \"date_calc\" from est.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            self.X_train_observed[loc] = self.X_train_observed[loc][features]\n",
    "            self.X_train_estimated[loc] = self.X_train_estimated[loc][features]\n",
    "            self.X_test_estimated[loc] = self.X_test_estimated[loc][features]\n",
    "\n",
    "    def add_type(self):\n",
    "        \"\"\"\n",
    "        0: Estimated data\n",
    "        1: Observed data\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            type_vec_X_tr = [1] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['type'] = type_vec_X_tr\n",
    "\n",
    "            type_vec_X_tr_e = [0] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['type'] = type_vec_X_tr_e\n",
    "\n",
    "            type_vec_X_te = [0] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['type'] = type_vec_X_te\n",
    "\n",
    "\n",
    "    def add_location(self):\n",
    "        \"\"\"\n",
    "        Adds a categorical feature \"location\" equal to the input string location.\n",
    "        \"\"\"\n",
    "        for loc in ['a','b','c']:\n",
    "            loc_vec_X_tr = [loc] * len(self.X_train_observed[loc])\n",
    "            self.X_train_observed[loc]['location'] = loc_vec_X_tr\n",
    "\n",
    "            loc_vec_X_tr_e = [loc] * len(self.X_train_estimated[loc])\n",
    "            self.X_train_estimated[loc]['location'] = loc_vec_X_tr_e\n",
    "\n",
    "            loc_vec_X_te = [loc] * len(self.X_test_estimated[loc])\n",
    "            self.X_test_estimated[loc]['location'] = loc_vec_X_te\n",
    "\n",
    "    def remove_nans(self, feature):\n",
    "        for loc in ['a','b','c']:\n",
    "            cols = self.X_train_observed['a'].columns\n",
    "            if feature in cols:\n",
    "                self.X_train_observed[loc] = self.X_train_observed[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_train_estimated[loc] = self.X_train_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "                self.X_test_estimated[loc] = self.X_test_estimated[loc].dropna(subset = [feature], how = 'all')\n",
    "            else:\n",
    "                print(\"Feature not in data frame.\")\n",
    "\n",
    "    def combine_obs_est(self):\n",
    "        \"\"\"\n",
    "        Concatinates the estimated and observed data. \n",
    "        Removes data_calc from est.\n",
    "        \"\"\"\n",
    "\n",
    "        obs_a = self.X_train_observed['a']\n",
    "        est_a = self.X_train_estimated['a']\n",
    "\n",
    "        obs_b = self.X_train_observed['b']\n",
    "        est_b = self.X_train_estimated['b']\n",
    "\n",
    "        obs_c = self.X_train_observed['c']\n",
    "        est_c = self.X_train_estimated['c']\n",
    "\n",
    "        self.X_train = {\n",
    "        'a':pd.concat([obs_a, est_a]),\n",
    "        'b':pd.concat([obs_b, est_b]),\n",
    "        'c':pd.concat([obs_c, est_c])\n",
    "        }\n",
    "\n",
    "        self.X_train['a'] = self.X_train['a'].reset_index(drop=True)\n",
    "        self.X_train['b'] = self.X_train['b'].reset_index(drop=True)\n",
    "        self.X_train['c'] = self.X_train['c'].reset_index(drop=True)\n",
    "\n",
    "        self.X_train['a'], self.Y_train['a'] = match_X_Y(self.X_train['a'], self.Y_train['a'])\n",
    "        self.X_train['b'], self.Y_train['b'] = match_X_Y(self.X_train['b'], self.Y_train['b'])\n",
    "        self.X_train['c'], self.Y_train['c'] = match_X_Y(self.X_train['c'], self.Y_train['c'])\n",
    "    \n",
    "    def train_test(self):\n",
    "        \"\"\"\n",
    "        Vanilla split. \n",
    "        \"\"\"\n",
    "        X_a = self.X_train['a']\n",
    "        X_b = self.X_train['b']\n",
    "        X_c = self.X_train['c']\n",
    "\n",
    "        y_a = self.Y_train['a']\n",
    "        y_b = self.Y_train['b']\n",
    "        y_c = self.Y_train['c']\n",
    "\n",
    "        y_train = pd.concat([y_a, y_b, y_c])\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "        X_train = pd.concat([X_a, X_b, X_c])\n",
    "        X_test = pd.concat([self.X_test_estimated['a'], self.X_test_estimated['b'],self.X_test_estimated['c']])\n",
    "        \n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "    def scale_y_train(self, k_b = 5, k_c = 6):\n",
    "\n",
    "        self.Y_train['b'] = self.Y_train['b'] * k_b \n",
    "        self.Y_train['c'] = self.Y_train['c']* k_c\n",
    "\n",
    "    def drop_bad_data(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            y_ind = get_constant_indices(self.Y_train[loc])\n",
    "            self.Y_train[loc].drop(y_ind, errors='ignore')\n",
    "            self.X_train[loc].drop(y_ind, errors='ignore')\n",
    "\n",
    "\n",
    "    def cyclic_time_encoding(self):\n",
    "        for loc in ['a', 'b', 'c']:\n",
    "            for time_feature in [\"time\", \"date_forecast\"]:\n",
    "                if time_feature in self.X_train[loc].columns:\n",
    "                    self.X_train[loc]['sin_hour'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['sin_month'] = np.sin(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_train[loc]['cos_hour'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.hour/24)\n",
    "                    self.X_train[loc]['cos_month'] = np.cos(2*np.pi*self.X_train[loc][time_feature].dt.month/12)\n",
    "                if time_feature in self.X_test_estimated[loc].columns:    \n",
    "                    self.X_test_estimated[loc]['sin_hour'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['sin_month'] = np.sin(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "                    self.X_test_estimated[loc]['cos_hour'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.hour/24)\n",
    "                    self.X_test_estimated[loc]['cos_month'] = np.cos(2*np.pi*self.X_test_estimated[loc][time_feature].dt.month/12)\n",
    "\n",
    "#Helper functions\n",
    "\n",
    "def match_X_Y(X,Y):\n",
    "    \"\"\" \n",
    "    date_forecast and time must be unique!\n",
    "    Matches the timestamps of X to the timestamps of Y. \n",
    "    Makes sure that the length of X and Y are equal.\n",
    "    \"\"\"\n",
    "    Y = Y.dropna()\n",
    "    X = X.rename(columns={'date_forecast': 'time'})\n",
    "    merge_df = Y.merge(X, on=\"time\", how='inner')\n",
    "    Y = merge_df['pv_measurement']\n",
    "    X = merge_df.drop(columns = ['pv_measurement'])\n",
    "    return X,Y\n",
    "\n",
    "def to_hourly(df):\n",
    "    df['date_forecast']\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('H').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def make_categorical(data, feature_list):\n",
    "    for feature in feature_list:\n",
    "        data[feature] = data[feature].astype('category')\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def remap(x):\n",
    "    if x<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_constant_indices(ser):\n",
    "    mask = (ser != 0)\n",
    "    constant_periods = ser[mask].groupby((ser[mask] != ser[mask].shift()).cumsum()).cumcount().add(1)\n",
    "    \n",
    "    drop_mask = constant_periods >= 12\n",
    "    return constant_periods[drop_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['date_forecast', 'absolute_humidity_2m:gm3',\n",
    "       'clear_sky_energy_1h:J', 'clear_sky_rad:W',\n",
    "       'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K',\n",
    "       'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J',\n",
    "       'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm',\n",
    "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
    "       'fresh_snow_6h:cm', 'is_in_shadow:idx', 'is_day:idx', \n",
    "       'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p',\n",
    "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
    "       'snow_depth:cm', 'snow_drift:idx',\n",
    "       'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d',\n",
    "       'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
    "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
    "       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "made_features = ['location', 'type', 'is_day:idx', 'is_in_shadow:idx', 'dew_or_rime:idx']\n",
    "\n",
    "drop_feature = 'diffuse_rad:W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection = DataSet()\n",
    "data_collection.select_features(selected_features)\n",
    "data_collection.resample_to_hourly()\n",
    "data_collection.remove_nans(drop_feature)\n",
    "data_collection.add_location()\n",
    "data_collection.add_type()\n",
    "data_collection.combine_obs_est()\n",
    "data_collection.drop_bad_data()\n",
    "data_collection.cyclic_time_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = data_collection.X_train['a']\n",
    "X_b = data_collection.X_train['b']\n",
    "X_c = data_collection.X_train['c']\n",
    "\n",
    "y_a = data_collection.Y_train['a']\n",
    "y_b = data_collection.Y_train['b']\n",
    "y_c = data_collection.Y_train['c']\n",
    "\n",
    "for f in made_features:\n",
    "    if f not in ['location', 'type']:\n",
    "        X_a[f] = X_a[f].map(remap)\n",
    "        X_b[f] = X_b[f].map(remap)\n",
    "        X_c[f] = X_c[f].map(remap)\n",
    "\n",
    "make_categorical(X_a,made_features)\n",
    "make_categorical(X_b,made_features)\n",
    "make_categorical(X_c,made_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_cols = ['location', 'time']\n",
    "\n",
    "df_a = pd.concat([X_a, y_a], axis=1).drop(columns=drop_cols)\n",
    "df_b = pd.concat([X_b, y_b], axis=1).drop(columns=drop_cols)\n",
    "df_c = pd.concat([X_c, y_c], axis=1).drop(columns=drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 246\n",
    "\n",
    "data = dict()\n",
    "\n",
    "# sample 50% of the data for each building with type = 0\n",
    "df_a_tune = df_a[df_a['type'] == 0].sample(frac=0.5, random_state=seed)\n",
    "df_b_tune = df_b[df_b['type'] == 0].sample(frac=0.5, random_state=seed)   \n",
    "df_c_tune = df_c[df_c['type'] == 0].sample(frac=0.5, random_state=seed)\n",
    "\n",
    "# drop these rows from the original data\n",
    "df_a_train = df_a.drop(df_a_tune.index)\n",
    "df_b_train = df_b.drop(df_b_tune.index)\n",
    "df_c_train = df_c.drop(df_c_tune.index)\n",
    "\n",
    "data['a'] = [df_a_train, df_a_tune]\n",
    "data['b'] = [df_b_train, df_b_tune]\n",
    "data['c'] = [df_c_train, df_c_tune]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 hours (per model)\n",
    "time_in_sek = 60*60*2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231116_082447/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231116_082447/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.8.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\n",
      "Disk Space Avail:   35.80 GB / 339.99 GB (10.5%)\n",
      "Train Data Rows:    31864\n",
      "Train Data Columns: 47\n",
      "Tuning Data Rows:    2197\n",
      "Tuning Data Columns: 47\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 650.65332, 1179.83452)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20282.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.54 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['elevation:m', 'snow_drift:idx']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t\t('float', [])    : 41 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 41 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\t\t('int', ['bool']) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t0.1s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.27 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7199.85s of the 7199.84s of remaining time.\n",
      "\t-170.5398\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t17.73s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7180.33s of the 7180.33s of remaining time.\n",
      "\t-170.2365\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t18.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7160.75s of the 7160.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "/home/dashuo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:369: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/dashuo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:369: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/dashuo/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:369: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "\t-91.6585\t = Validation score   (-mean_absolute_error)\n",
      "\t54.59s\t = Training   runtime\n",
      "\t63.89s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7087.44s of the 7087.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.8131\t = Validation score   (-mean_absolute_error)\n",
      "\t64.32s\t = Training   runtime\n",
      "\t38.5s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 7013.5s of the 7013.49s of remaining time.\n",
      "\t-112.6177\t = Validation score   (-mean_absolute_error)\n",
      "\t20.96s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6990.88s of the 6990.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-105.1783\t = Validation score   (-mean_absolute_error)\n",
      "\t334.18s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6654.4s of the 6654.4s of remaining time.\n",
      "\t-112.2127\t = Validation score   (-mean_absolute_error)\n",
      "\t5.35s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6647.39s of the 6647.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-109.9873\t = Validation score   (-mean_absolute_error)\n",
      "\t51.92s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6592.2s of the 6592.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-105.5894\t = Validation score   (-mean_absolute_error)\n",
      "\t89.45s\t = Training   runtime\n",
      "\t2.95s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6497.97s of the 6497.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-98.0603\t = Validation score   (-mean_absolute_error)\n",
      "\t124.26s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6371.66s of the 6371.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-96.7658\t = Validation score   (-mean_absolute_error)\n",
      "\t161.68s\t = Training   runtime\n",
      "\t84.26s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6184.33s of the 6184.33s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.5613\t = Validation score   (-mean_absolute_error)\n",
      "\t109.18s\t = Training   runtime\n",
      "\t127.3s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6112.06s of the 6112.05s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.4679\t = Validation score   (-mean_absolute_error)\n",
      "\t121.69s\t = Training   runtime\n",
      "\t68.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6040.9s of the 6040.9s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.7195\t = Validation score   (-mean_absolute_error)\n",
      "\t671.84s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5701.18s of the 5701.18s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-109.2595\t = Validation score   (-mean_absolute_error)\n",
      "\t101.17s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 5649.27s of the 5649.27s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.9263\t = Validation score   (-mean_absolute_error)\n",
      "\t192.25s\t = Training   runtime\n",
      "\t9.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5539.97s of the 5539.97s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.3511\t = Validation score   (-mean_absolute_error)\n",
      "\t256.21s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5405.88s of the 5405.87s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.7628\t = Validation score   (-mean_absolute_error)\n",
      "\t327.75s\t = Training   runtime\n",
      "\t183.65s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5205.24s of the 5205.24s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.3144\t = Validation score   (-mean_absolute_error)\n",
      "\t166.54s\t = Training   runtime\n",
      "\t200.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5126.24s of the 5126.24s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.183\t = Validation score   (-mean_absolute_error)\n",
      "\t189.13s\t = Training   runtime\n",
      "\t114.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5042.86s of the 5042.86s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.652\t = Validation score   (-mean_absolute_error)\n",
      "\t1004.69s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4707.7s of the 4707.7s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-109.0019\t = Validation score   (-mean_absolute_error)\n",
      "\t150.13s\t = Training   runtime\n",
      "\t1.65s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4655.55s of the 4655.55s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.6682\t = Validation score   (-mean_absolute_error)\n",
      "\t278.94s\t = Training   runtime\n",
      "\t12.64s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 4562.22s of the 4562.21s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.2953\t = Validation score   (-mean_absolute_error)\n",
      "\t385.99s\t = Training   runtime\n",
      "\t1.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4430.12s of the 4430.12s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.8981\t = Validation score   (-mean_absolute_error)\n",
      "\t480.79s\t = Training   runtime\n",
      "\t264.39s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4240.05s of the 4240.05s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.2554\t = Validation score   (-mean_absolute_error)\n",
      "\t217.48s\t = Training   runtime\n",
      "\t256.44s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4166.9s of the 4166.89s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.3312\t = Validation score   (-mean_absolute_error)\n",
      "\t248.49s\t = Training   runtime\n",
      "\t154.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4090.09s of the 4090.09s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.6171\t = Validation score   (-mean_absolute_error)\n",
      "\t1326.25s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3766.47s of the 3766.46s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-108.872\t = Validation score   (-mean_absolute_error)\n",
      "\t197.91s\t = Training   runtime\n",
      "\t2.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3715.34s of the 3715.34s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-103.8981\t = Validation score   (-mean_absolute_error)\n",
      "\t384.72s\t = Training   runtime\n",
      "\t24.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3600.63s of the 3600.63s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.3585\t = Validation score   (-mean_absolute_error)\n",
      "\t503.8s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3480.03s of the 3480.03s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.9215\t = Validation score   (-mean_absolute_error)\n",
      "\t634.79s\t = Training   runtime\n",
      "\t330.64s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3287.36s of the 3287.35s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.2525\t = Validation score   (-mean_absolute_error)\n",
      "\t267.33s\t = Training   runtime\n",
      "\t317.37s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3212.27s of the 3212.27s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.2688\t = Validation score   (-mean_absolute_error)\n",
      "\t306.23s\t = Training   runtime\n",
      "\t213.86s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3130.89s of the 3130.89s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.6233\t = Validation score   (-mean_absolute_error)\n",
      "\t1646.05s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2808.58s of the 2808.57s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-108.7237\t = Validation score   (-mean_absolute_error)\n",
      "\t246.04s\t = Training   runtime\n",
      "\t2.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2756.91s of the 2756.91s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-103.8802\t = Validation score   (-mean_absolute_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t479.57s\t = Training   runtime\n",
      "\t31.7s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2651.99s of the 2651.99s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.541\t = Validation score   (-mean_absolute_error)\n",
      "\t620.89s\t = Training   runtime\n",
      "\t1.9s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2531.82s of the 2531.81s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.8442\t = Validation score   (-mean_absolute_error)\n",
      "\t786.3s\t = Training   runtime\n",
      "\t416.08s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2331.17s of the 2331.17s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.1536\t = Validation score   (-mean_absolute_error)\n",
      "\t317.54s\t = Training   runtime\n",
      "\t379.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2252.57s of the 2252.57s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.2429\t = Validation score   (-mean_absolute_error)\n",
      "\t358.49s\t = Training   runtime\n",
      "\t232.25s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2180.21s of the 2180.21s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.6122\t = Validation score   (-mean_absolute_error)\n",
      "\t1965.84s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1857.79s of the 1857.79s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-108.8813\t = Validation score   (-mean_absolute_error)\n",
      "\t294.21s\t = Training   runtime\n",
      "\t3.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1805.89s of the 1805.89s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.07\t = Validation score   (-mean_absolute_error)\n",
      "\t562.96s\t = Training   runtime\n",
      "\t34.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1713.44s of the 1713.44s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.6575\t = Validation score   (-mean_absolute_error)\n",
      "\t750.29s\t = Training   runtime\n",
      "\t2.27s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1580.89s of the 1580.89s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.7983\t = Validation score   (-mean_absolute_error)\n",
      "\t936.88s\t = Training   runtime\n",
      "\t487.22s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1378.02s of the 1378.02s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-91.1458\t = Validation score   (-mean_absolute_error)\n",
      "\t368.14s\t = Training   runtime\n",
      "\t438.82s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1297.2s of the 1297.2s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-99.1905\t = Validation score   (-mean_absolute_error)\n",
      "\t416.21s\t = Training   runtime\n",
      "\t287.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1211.71s of the 1211.71s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.5463\t = Validation score   (-mean_absolute_error)\n",
      "\t2287.62s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 887.66s of the 887.66s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-108.7659\t = Validation score   (-mean_absolute_error)\n",
      "\t342.38s\t = Training   runtime\n",
      "\t3.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 835.48s of the 835.48s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.0103\t = Validation score   (-mean_absolute_error)\n",
      "\t647.28s\t = Training   runtime\n",
      "\t37.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 741.58s of the 741.57s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-97.8036\t = Validation score   (-mean_absolute_error)\n",
      "\t868.81s\t = Training   runtime\n",
      "\t2.66s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 619.56s of the 619.56s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.8534\t = Validation score   (-mean_absolute_error)\n",
      "\t1085.66s\t = Training   runtime\n",
      "\t586.59s\t = Validation runtime\n",
      "Completed 7/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 719.98s of the 403.44s of remaining time.\n",
      "\t-90.2284\t = Validation score   (-mean_absolute_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6796.86s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231116_082447/\")\n"
     ]
    }
   ],
   "source": [
    "label = 'pv_measurement'\n",
    "predictor_a = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['a'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['a'][1],\n",
    "    use_bag_holdout= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231116_101804/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231116_101804/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.8.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\n",
      "Disk Space Avail:   25.30 GB / 339.99 GB (7.4%)\n",
      "Train Data Rows:    31019\n",
      "Train Data Columns: 47\n",
      "Tuning Data Rows:    1800\n",
      "Tuning Data Columns: 47\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 99.69624, 196.54802)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17326.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.3 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['elevation:m']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t\t('float', [])    : 42 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\t\t('int', ['bool']) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t0.2s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.17 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7199.82s of the 7199.82s of remaining time.\n",
      "\t-30.5762\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t17.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7181.33s of the 7181.33s of remaining time.\n",
      "\t-30.6245\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t19.88s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7160.23s of the 7160.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.5998\t = Validation score   (-mean_absolute_error)\n",
      "\t47.9s\t = Training   runtime\n",
      "\t59.59s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7098.66s of the 7098.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.4243\t = Validation score   (-mean_absolute_error)\n",
      "\t60.54s\t = Training   runtime\n",
      "\t45.93s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 7028.85s of the 7028.84s of remaining time.\n",
      "\t-16.8377\t = Validation score   (-mean_absolute_error)\n",
      "\t21.1s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 7006.45s of the 7006.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-16.506\t = Validation score   (-mean_absolute_error)\n",
      "\t315.8s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6688.78s of the 6688.78s of remaining time.\n",
      "\t-16.2079\t = Validation score   (-mean_absolute_error)\n",
      "\t4.55s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6682.83s of the 6682.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.436\t = Validation score   (-mean_absolute_error)\n",
      "\t47.4s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6633.14s of the 6633.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.6074\t = Validation score   (-mean_absolute_error)\n",
      "\t192.36s\t = Training   runtime\n",
      "\t68.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6426.98s of the 6426.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.7558\t = Validation score   (-mean_absolute_error)\n",
      "\t195.22s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6229.67s of the 6229.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.5535\t = Validation score   (-mean_absolute_error)\n",
      "\t154.47s\t = Training   runtime\n",
      "\t59.74s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6057.83s of the 6057.83s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.5251\t = Validation score   (-mean_absolute_error)\n",
      "\t97.16s\t = Training   runtime\n",
      "\t121.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5991.76s of the 5991.76s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3098\t = Validation score   (-mean_absolute_error)\n",
      "\t115.53s\t = Training   runtime\n",
      "\t103.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5920.37s of the 5920.37s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-16.4631\t = Validation score   (-mean_absolute_error)\n",
      "\t632.73s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5601.69s of the 5601.69s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3561\t = Validation score   (-mean_absolute_error)\n",
      "\t95.51s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 5551.11s of the 5551.11s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3887\t = Validation score   (-mean_absolute_error)\n",
      "\t383.59s\t = Training   runtime\n",
      "\t131.42s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5343.84s of the 5343.84s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.686\t = Validation score   (-mean_absolute_error)\n",
      "\t422.13s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5114.84s of the 5114.84s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.5084\t = Validation score   (-mean_absolute_error)\n",
      "\t302.79s\t = Training   runtime\n",
      "\t131.11s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4940.11s of the 4940.11s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.5462\t = Validation score   (-mean_absolute_error)\n",
      "\t145.94s\t = Training   runtime\n",
      "\t181.63s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4872.58s of the 4872.57s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3066\t = Validation score   (-mean_absolute_error)\n",
      "\t171.31s\t = Training   runtime\n",
      "\t161.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4798.32s of the 4798.32s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-16.4738\t = Validation score   (-mean_absolute_error)\n",
      "\t947.13s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4481.78s of the 4481.78s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.2079\t = Validation score   (-mean_absolute_error)\n",
      "\t143.69s\t = Training   runtime\n",
      "\t1.6s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4430.85s of the 4430.85s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3376\t = Validation score   (-mean_absolute_error)\n",
      "\t575.0s\t = Training   runtime\n",
      "\t195.8s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 4219.55s of the 4219.55s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.8074\t = Validation score   (-mean_absolute_error)\n",
      "\t589.97s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4049.39s of the 4049.39s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.4566\t = Validation score   (-mean_absolute_error)\n",
      "\t451.61s\t = Training   runtime\n",
      "\t201.47s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3868.07s of the 3868.06s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.5483\t = Validation score   (-mean_absolute_error)\n",
      "\t194.77s\t = Training   runtime\n",
      "\t240.86s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3798.24s of the 3798.24s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3225\t = Validation score   (-mean_absolute_error)\n",
      "\t226.79s\t = Training   runtime\n",
      "\t219.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3721.72s of the 3721.71s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-16.4439\t = Validation score   (-mean_absolute_error)\n",
      "\t1263.45s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3403.08s of the 3403.08s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.1587\t = Validation score   (-mean_absolute_error)\n",
      "\t190.91s\t = Training   runtime\n",
      "\t2.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3352.69s of the 3352.69s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3024\t = Validation score   (-mean_absolute_error)\n",
      "\t764.72s\t = Training   runtime\n",
      "\t260.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3140.84s of the 3140.83s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.8824\t = Validation score   (-mean_absolute_error)\n",
      "\t774.4s\t = Training   runtime\n",
      "\t1.53s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2953.53s of the 2953.52s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.4728\t = Validation score   (-mean_absolute_error)\n",
      "\t606.76s\t = Training   runtime\n",
      "\t267.65s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2764.43s of the 2764.42s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.4936\t = Validation score   (-mean_absolute_error)\n",
      "\t242.66s\t = Training   runtime\n",
      "\t299.65s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2692.8s of the 2692.79s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.3229\t = Validation score   (-mean_absolute_error)\n",
      "\t283.99s\t = Training   runtime\n",
      "\t271.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2614.12s of the 2614.12s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-16.4525\t = Validation score   (-mean_absolute_error)\n",
      "\t1579.92s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2295.57s of the 2295.56s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.1589\t = Validation score   (-mean_absolute_error)\n",
      "\t237.61s\t = Training   runtime\n",
      "\t2.63s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2245.6s of the 2245.6s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.2693\t = Validation score   (-mean_absolute_error)\n",
      "\t954.25s\t = Training   runtime\n",
      "\t324.65s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2030.22s of the 2030.21s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.9306\t = Validation score   (-mean_absolute_error)\n",
      "\t984.88s\t = Training   runtime\n",
      "\t1.92s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1816.62s of the 1816.62s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.4551\t = Validation score   (-mean_absolute_error)\n",
      "\t754.5s\t = Training   runtime\n",
      "\t338.8s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1621.33s of the 1621.33s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.4713\t = Validation score   (-mean_absolute_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t297.53s\t = Training   runtime\n",
      "\t371.61s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1537.14s of the 1537.14s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.329\t = Validation score   (-mean_absolute_error)\n",
      "\t344.02s\t = Training   runtime\n",
      "\t343.78s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1447.95s of the 1447.95s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-16.4714\t = Validation score   (-mean_absolute_error)\n",
      "\t1920.33s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1104.92s of the 1104.91s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.2013\t = Validation score   (-mean_absolute_error)\n",
      "\t288.37s\t = Training   runtime\n",
      "\t3.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1049.88s of the 1049.88s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.2445\t = Validation score   (-mean_absolute_error)\n",
      "\t1159.18s\t = Training   runtime\n",
      "\t397.99s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 813.38s of the 813.38s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.9073\t = Validation score   (-mean_absolute_error)\n",
      "\t1205.42s\t = Training   runtime\n",
      "\t2.3s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 589.72s of the 589.72s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.4539\t = Validation score   (-mean_absolute_error)\n",
      "\t915.29s\t = Training   runtime\n",
      "\t416.78s\t = Validation runtime\n",
      "Completed 6/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 719.98s of the 373.7s of remaining time.\n",
      "\t-12.7457\t = Validation score   (-mean_absolute_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6826.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231116_101804/\")\n"
     ]
    }
   ],
   "source": [
    "predictor_b = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['b'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['b'][1],\n",
    "    use_bag_holdout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231116_121151/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231116_121151/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.8.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\n",
      "Disk Space Avail:   13.29 GB / 339.99 GB (3.9%)\n",
      "Train Data Rows:    24606\n",
      "Train Data Columns: 47\n",
      "Tuning Data Rows:    1465\n",
      "Tuning Data Columns: 47\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 79.70535, 168.37633)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14196.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['elevation:m', 'snow_drift:idx']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t\t('float', [])    : 41 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 41 | ['absolute_humidity_2m:gm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_point_2m:K', ...]\n",
      "\t\t('int', ['bool']) :  4 | ['dew_or_rime:idx', 'is_in_shadow:idx', 'is_day:idx', 'type']\n",
      "\t0.1s = Fit runtime\n",
      "\t45 features in original data used to generate 45 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.8 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7199.87s of the 7199.87s of remaining time.\n",
      "\t-20.8779\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t12.51s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7186.34s of the 7186.34s of remaining time.\n",
      "\t-20.7787\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t11.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7174.16s of the 7174.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.6025\t = Validation score   (-mean_absolute_error)\n",
      "\t46.66s\t = Training   runtime\n",
      "\t42.45s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7115.92s of the 7115.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.2\t = Validation score   (-mean_absolute_error)\n",
      "\t61.44s\t = Training   runtime\n",
      "\t42.36s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 7042.75s of the 7042.75s of remaining time.\n",
      "\t-16.8133\t = Validation score   (-mean_absolute_error)\n",
      "\t15.48s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 7026.35s of the 7026.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4489\t = Validation score   (-mean_absolute_error)\n",
      "\t329.41s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6694.53s of the 6694.53s of remaining time.\n",
      "\t-15.6165\t = Validation score   (-mean_absolute_error)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6690.43s of the 6690.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.5703\t = Validation score   (-mean_absolute_error)\n",
      "\t42.1s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6645.83s of the 6645.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.7789\t = Validation score   (-mean_absolute_error)\n",
      "\t186.51s\t = Training   runtime\n",
      "\t43.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6448.19s of the 6448.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.1541\t = Validation score   (-mean_absolute_error)\n",
      "\t103.16s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6342.93s of the 6342.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4775\t = Validation score   (-mean_absolute_error)\n",
      "\t169.76s\t = Training   runtime\n",
      "\t48.27s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6153.04s of the 6153.04s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5668\t = Validation score   (-mean_absolute_error)\n",
      "\t95.44s\t = Training   runtime\n",
      "\t99.29s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6088.12s of the 6088.12s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.1276\t = Validation score   (-mean_absolute_error)\n",
      "\t122.41s\t = Training   runtime\n",
      "\t75.62s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6017.15s of the 6017.15s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-13.3955\t = Validation score   (-mean_absolute_error)\n",
      "\t648.07s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5696.47s of the 5696.47s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.5631\t = Validation score   (-mean_absolute_error)\n",
      "\t82.16s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 5653.43s of the 5653.43s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.8469\t = Validation score   (-mean_absolute_error)\n",
      "\t293.35s\t = Training   runtime\n",
      "\t54.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5537.34s of the 5537.34s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0076\t = Validation score   (-mean_absolute_error)\n",
      "\t208.92s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5429.07s of the 5429.07s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4505\t = Validation score   (-mean_absolute_error)\n",
      "\t344.22s\t = Training   runtime\n",
      "\t102.15s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5231.09s of the 5231.09s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5904\t = Validation score   (-mean_absolute_error)\n",
      "\t148.87s\t = Training   runtime\n",
      "\t147.71s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5158.8s of the 5158.8s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0893\t = Validation score   (-mean_absolute_error)\n",
      "\t181.61s\t = Training   runtime\n",
      "\t109.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5083.85s of the 5083.85s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.3713\t = Validation score   (-mean_absolute_error)\n",
      "\t984.68s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4744.58s of the 4744.58s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.5302\t = Validation score   (-mean_absolute_error)\n",
      "\t126.32s\t = Training   runtime\n",
      "\t1.45s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4696.13s of the 4696.12s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.794\t = Validation score   (-mean_absolute_error)\n",
      "\t480.14s\t = Training   runtime\n",
      "\t88.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 4496.86s of the 4496.86s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.9877\t = Validation score   (-mean_absolute_error)\n",
      "\t321.64s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4381.42s of the 4381.42s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4855\t = Validation score   (-mean_absolute_error)\n",
      "\t522.58s\t = Training   runtime\n",
      "\t153.72s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4171.8s of the 4171.8s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.6009\t = Validation score   (-mean_absolute_error)\n",
      "\t199.24s\t = Training   runtime\n",
      "\t194.81s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4102.06s of the 4102.05s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0673\t = Validation score   (-mean_absolute_error)\n",
      "\t238.52s\t = Training   runtime\n",
      "\t149.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4028.27s of the 4028.27s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.3635\t = Validation score   (-mean_absolute_error)\n",
      "\t1295.92s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3714.88s of the 3714.88s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.46\t = Validation score   (-mean_absolute_error)\n",
      "\t165.66s\t = Training   runtime\n",
      "\t1.87s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3672.37s of the 3672.37s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.7027\t = Validation score   (-mean_absolute_error)\n",
      "\t654.02s\t = Training   runtime\n",
      "\t124.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3484.21s of the 3484.21s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.054\t = Validation score   (-mean_absolute_error)\n",
      "\t423.54s\t = Training   runtime\n",
      "\t1.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3379.34s of the 3379.34s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4396\t = Validation score   (-mean_absolute_error)\n",
      "\t676.59s\t = Training   runtime\n",
      "\t195.0s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3194.97s of the 3194.97s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5539\t = Validation score   (-mean_absolute_error)\n",
      "\t244.34s\t = Training   runtime\n",
      "\t244.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3128.71s of the 3128.71s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0811\t = Validation score   (-mean_absolute_error)\n",
      "\t295.11s\t = Training   runtime\n",
      "\t184.52s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3055.59s of the 3055.59s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.3485\t = Validation score   (-mean_absolute_error)\n",
      "\t1601.67s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2747.61s of the 2747.61s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4013\t = Validation score   (-mean_absolute_error)\n",
      "\t203.86s\t = Training   runtime\n",
      "\t2.29s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2705.94s of the 2705.94s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.6805\t = Validation score   (-mean_absolute_error)\n",
      "\t827.11s\t = Training   runtime\n",
      "\t158.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2517.12s of the 2517.12s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.9132\t = Validation score   (-mean_absolute_error)\n",
      "\t535.82s\t = Training   runtime\n",
      "\t1.7s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2402.07s of the 2402.07s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4832\t = Validation score   (-mean_absolute_error)\n",
      "\t830.69s\t = Training   runtime\n",
      "\t248.48s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/20\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2205.61s of the 2205.61s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5847\t = Validation score   (-mean_absolute_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t289.1s\t = Training   runtime\n",
      "\t293.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2137.92s of the 2137.92s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0548\t = Validation score   (-mean_absolute_error)\n",
      "\t349.32s\t = Training   runtime\n",
      "\t221.89s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2063.47s of the 2063.47s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.3504\t = Validation score   (-mean_absolute_error)\n",
      "\t1905.03s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1757.64s of the 1757.63s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4056\t = Validation score   (-mean_absolute_error)\n",
      "\t241.46s\t = Training   runtime\n",
      "\t2.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1716.4s of the 1716.4s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.6622\t = Validation score   (-mean_absolute_error)\n",
      "\t986.42s\t = Training   runtime\n",
      "\t177.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1540.37s of the 1540.37s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.8645\t = Validation score   (-mean_absolute_error)\n",
      "\t670.29s\t = Training   runtime\n",
      "\t2.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1403.04s of the 1403.03s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4564\t = Validation score   (-mean_absolute_error)\n",
      "\t987.48s\t = Training   runtime\n",
      "\t296.62s\t = Validation runtime\n",
      "Completed 6/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 719.99s of the 1205.81s of remaining time.\n",
      "\t-11.2468\t = Validation score   (-mean_absolute_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5994.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231116_121151/\")\n"
     ]
    }
   ],
   "source": [
    "predictor_c = TabularPredictor(label=label, eval_metric='mae').fit(\n",
    "    train_data = data['c'][0], \n",
    "    time_limit = time_in_sek,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=8,\n",
    "    num_stack_levels=0,\n",
    "    tuning_data = data['c'][1],\n",
    "    use_bag_holdout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t12.98s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t12.65s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL ...\n",
      "\t20.98s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t64.94s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL ...\n",
      "\t5.06s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 26.\n",
      "\t22.94s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t6.77s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t57.36s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t42.41s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.27s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 296.26s\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t12.08s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t13.57s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL ...\n",
      "\t21.97s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t66.24s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL ...\n",
      "\t4.56s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 26.\n",
      "\t23.52s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t28.69s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t108.24s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t43.19s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.28s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 375.4s\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t14.71s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t13.49s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL ...\n",
      "\t15.17s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused CatBoost_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 250, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, skip_oof=_skip_oof, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 442, in _fit_single\n",
      "    self.save_child(model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 792, in save_child\n",
      "    child.save(verbose=verbose)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1026, in save\n",
      "    save_pkl.save(path=file_path, object=self, verbose=verbose)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 47, in save_with_fn\n",
      "    pickle_fn(object, fout)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 25, in pickle_fn\n",
      "    return pickle.dump(o, buffer, protocol=4)\n",
      "OSError: [Errno 28] No space left on device\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused ExtraTreesMSE_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/ExtraTreesMSE_BAG_L1_FULL'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 228, in _fit\n",
      "    self.save_model_base(self.model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 998, in save_model_base\n",
      "    save_pkl.save(path=os.path.join(self.path + \"utils\", \"model_template.pkl\"), object=model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 41, in save_with_fn\n",
      "    os.makedirs(path_parent, exist_ok=True)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/ExtraTreesMSE_BAG_L1_FULL'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/NeuralNetFastAI_BAG_L1_FULL'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 228, in _fit\n",
      "    self.save_model_base(self.model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 998, in save_model_base\n",
      "    save_pkl.save(path=os.path.join(self.path + \"utils\", \"model_template.pkl\"), object=model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 41, in save_with_fn\n",
      "    os.makedirs(path_parent, exist_ok=True)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/NeuralNetFastAI_BAG_L1_FULL'\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused XGBoost_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/XGBoost_BAG_L1_FULL'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 228, in _fit\n",
      "    self.save_model_base(self.model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 998, in save_model_base\n",
      "    save_pkl.save(path=os.path.join(self.path + \"utils\", \"model_template.pkl\"), object=model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 41, in save_with_fn\n",
      "    os.makedirs(path_parent, exist_ok=True)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/XGBoost_BAG_L1_FULL'\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/NeuralNetTorch_BAG_L1_FULL'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 228, in _fit\n",
      "    self.save_model_base(self.model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 998, in save_model_base\n",
      "    save_pkl.save(path=os.path.join(self.path + \"utils\", \"model_template.pkl\"), object=model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 41, in save_with_fn\n",
      "    os.makedirs(path_parent, exist_ok=True)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/NeuralNetTorch_BAG_L1_FULL'\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\t\t[Errno 28] No space left on device\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1733, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1684, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 829, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 169, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 250, in _fit\n",
      "    self._fit_single(X=X, y=y, model_base=model_base, use_child_oof=use_child_oof, skip_oof=_skip_oof, **kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 442, in _fit_single\n",
      "    self.save_child(model_base)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 792, in save_child\n",
      "    child.save(verbose=verbose)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1026, in save\n",
      "    save_pkl.save(path=file_path, object=self, verbose=verbose)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 27, in save\n",
      "    save_with_fn(validated_path, object, pickle_fn, format=format, verbose=verbose, compression_fn=compression_fn, compression_fn_kwargs=compression_fn_kwargs)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 47, in save_with_fn\n",
      "    pickle_fn(object, fout)\n",
      "  File \"/home/dashuo/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py\", line 25, in pickle_fn\n",
      "    return pickle.dump(o, buffer, protocol=4)\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.24s\t = Training   runtime\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/WeightedEnsemble_L2_FULL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m predictor_a\u001b[38;5;241m.\u001b[39mrefit_full()\n\u001b[1;32m      2\u001b[0m predictor_b\u001b[38;5;241m.\u001b[39mrefit_full()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpredictor_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/tabular/predictor/predictor.py:2617\u001b[0m, in \u001b[0;36mTabularPredictor.refit_full\u001b[0;34m(self, model, set_best_to_refit_full)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_best\n\u001b[1;32m   2610\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   2612\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTo learn more, refer to the `.refit_full` method docstring which explains how \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_FULL\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m models differ from normal models.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2616\u001b[0m )\n\u001b[0;32m-> 2617\u001b[0m refit_full_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefit_ensemble_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m set_best_to_refit_full:\n\u001b[1;32m   2620\u001b[0m     model_full_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mget_model_full_dict()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/tabular/learner/abstract_learner.py:448\u001b[0m, in \u001b[0;36mAbstractTabularLearner.refit_ensemble_full\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrefit_ensemble_full\u001b[39m(\u001b[38;5;28mself\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefit_ensemble_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py:1334\u001b[0m, in \u001b[0;36mAbstractTrainer.refit_ensemble_full\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         ensemble_set_valid\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensemble_set_valid:\n\u001b[0;32m-> 1334\u001b[0m     models_trained_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefit_single_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble_set_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1336\u001b[0m     models_trained_full \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py:1268\u001b[0m, in \u001b[0;36mAbstractTrainer.refit_single_full\u001b[0;34m(self, X, y, X_val, y_val, X_unlabeled, models)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_full\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Skipping fit via cloning parent ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_model(model_full, stack_name\u001b[38;5;241m=\u001b[39mREFIT_FULL_NAME, level\u001b[38;5;241m=\u001b[39mlevel)\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m     models_trained \u001b[38;5;241m=\u001b[39m [model_full\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/core/trainer/abstract_trainer.py:1411\u001b[0m, in \u001b[0;36mAbstractTrainer.save_model\u001b[0;34m(self, model, reduce_memory)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     model\u001b[38;5;241m.\u001b[39mreduce_memory_size(remove_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, requires_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m-> 1411\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:1022\u001b[0m, in \u001b[0;36mBaggedEnsembleModel.save\u001b[0;34m(self, path, verbose, save_oof, save_children)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_child_model_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels)\n\u001b[0;32m-> 1022\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m _models\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/core/models/abstract/abstract_model.py:1026\u001b[0m, in \u001b[0;36mAbstractModel.save\u001b[0;34m(self, path, verbose)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiler\u001b[38;5;241m.\u001b[39msave_in_pkl:\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Don't save model in pkl\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m \u001b[43msave_pkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m _model\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py:27\u001b[0m, in \u001b[0;36msave\u001b[0;34m(path, object, format, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpickle_fn\u001b[39m(o, buffer):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mdump(o, buffer, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43msave_with_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidated_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_fn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/autogluon/common/savers/save_pkl.py:41\u001b[0m, in \u001b[0;36msave_with_fn\u001b[0;34m(path, object, pickle_fn, format, verbose, compression_fn, compression_fn_kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_parent \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     40\u001b[0m     path_parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Allows saving to working directory root without crashing\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression_fn_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     compression_fn_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: 'AutogluonModels/ag-20231116_121151/models/WeightedEnsemble_L2_FULL'"
     ]
    }
   ],
   "source": [
    "predictor_a.refit_full()\n",
    "predictor_b.refit_full()\n",
    "predictor_c.refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_a.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_b.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_c.leaderboard(silent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = data_collection.X_test_estimated['a'].drop(columns=['location', 'date_forecast'])\n",
    "test_b = data_collection.X_test_estimated['b'].drop(columns=['location', 'date_forecast'])\n",
    "test_c = data_collection.X_test_estimated['c'].drop(columns=['location', 'date_forecast'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_a = predictor_a.predict(test_a)\n",
    "y_pred_b = predictor_b.predict(test_b)\n",
    "y_pred_c = predictor_c.predict(test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.concat([y_pred_a, y_pred_b, y_pred_c]).reset_index(drop=True)\n",
    "final_pred_AutoGluon = ReLU(final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['date_forecast', 'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_6h:cm', 'is_day:idx',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_depth:cm',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "made_features = ['location', 'type', 'is_day:idx', 'is_in_shadow:idx', 'dew_or_rime:idx']\n",
    "\n",
    "drop_feature = 'diffuse_rad:W'\n",
    "\n",
    "\n",
    "#Loading all data\n",
    "data_collection = DataSet()\n",
    "#Preprocessing\n",
    "data_collection.select_features(selected_features)\n",
    "data_collection.resample_to_hourly()\n",
    "data_collection.remove_nans(drop_feature)\n",
    "data_collection.add_location()\n",
    "data_collection.add_type()\n",
    "data_collection.combine_obs_est()\n",
    "data_collection.drop_bad_data()\n",
    "data_collection.cyclic_time_encoding()\n",
    "\n",
    "k_b = 5\n",
    "k_c = 6\n",
    "data_collection.scale_y_train(k_b = k_b, k_c = k_c)\n",
    "\n",
    "X_train, X_test, y_train = data_collection.train_test()\n",
    "\n",
    "for f in made_features:\n",
    "    if f not in ['location', 'type']:\n",
    "        X_train[f] = X_train[f].map(remap)\n",
    "        X_test[f] = X_test[f].map(remap)\n",
    "\n",
    "make_categorical(X_train,made_features)\n",
    "X_train = X_train.drop('time', axis=1)\n",
    "\n",
    "make_categorical(X_test,made_features)\n",
    "X_test = X_test.drop('date_forecast', axis=1)\n",
    "\n",
    "train_pool = cb.Pool(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cat_features = made_features\n",
    ")\n",
    "test_pool = cb.Pool(\n",
    "    X_test,\n",
    "    cat_features = made_features\n",
    ")\n",
    "\n",
    "model = cb.CatBoostRegressor(\n",
    "    iterations = 10000,\n",
    "    depth = 9,\n",
    "    learning_rate =0.005,\n",
    "    loss_function ='MAE',\n",
    "    cat_features = made_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(train_pool, silent=True)\n",
    "# make the prediction using the resulting model\n",
    "preds = model.predict(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale back\n",
    "length = int((X_test.shape[0]/3))\n",
    "pred_a = preds[:length]\n",
    "pred_b = preds[length:2*length] / k_b\n",
    "pred_c = preds[2*length:3*length] / k_c\n",
    "preds = np.concatenate([pred_a,pred_b, pred_c])\n",
    "#Drop negative values\n",
    "final_pred_cb = ReLU(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining for final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_AutoGluon = pd.DataFrame({'predictions':final_pred_AutoGluon})\n",
    "final_pred_AutoGluon['predictions'] = final_pred_AutoGluon['predictions'].apply(lambda x: 0 if x < 5 else x)\n",
    "\n",
    "final_pred_cb = pd.DataFrame({'predictions':final_pred_cb})\n",
    "final_pred_cb['predictions'] = final_pred_cb['predictions'].apply(lambda x: 0 if x < 5 else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = 0.5*(final_pred_AutoGluon + final_pred_cb)\n",
    "\n",
    "final_pred = final_pred.reset_index()\n",
    "final_pred = final_pred.rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred.to_csv('Short_notebook_2.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
